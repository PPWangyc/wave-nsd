{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67510ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "from pycocotools.coco import COCO\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DATA_DIR=\"../data/NSD-raw\"\n",
    "SUBJECT=[1,2,5,7]\n",
    "FILTERED_SUBJECT=[3,4,6,8]\n",
    "NUM_SESSION=40\n",
    "PREP_TYPE=\"betas_fithrf_GLMdenoise_RR\"\n",
    "PREP_NAME=\"betas\"\n",
    "\n",
    "OUTPUT_DIR=\"../data/NSD-processed\"\n",
    "\n",
    "stimuli_img = os.path.join(DATA_DIR, \"stimuli\", \"nsd_stimuli.hdf5\")\n",
    "stimuli_info = os.path.join(DATA_DIR, \"stimuli\", \"nsd_stim_info_merged.csv\")\n",
    "train_coco_annotations = os.path.join(DATA_DIR, \"stimuli\", \"annotations\", \"instances_train2017.json\")\n",
    "val_coco_annotations = os.path.join(DATA_DIR, \"stimuli\", \"annotations\", \"instances_val2017.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6362103",
   "metadata": {},
   "source": [
    "Load stimuli information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "584f0000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of stimuli: 73000\n",
      "Index(['Unnamed: 0', 'cocoId', 'cocoSplit', 'cropBox', 'loss', 'nsdId',\n",
      "       'flagged', 'BOLD5000', 'shared1000', 'subject1', 'subject2', 'subject5',\n",
      "       'subject7', 'subject1_rep0', 'subject1_rep1', 'subject1_rep2',\n",
      "       'subject2_rep0', 'subject2_rep1', 'subject2_rep2', 'subject5_rep0',\n",
      "       'subject5_rep1', 'subject5_rep2', 'subject7_rep0', 'subject7_rep1',\n",
      "       'subject7_rep2'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# load csv\n",
    "stimuli_info = pd.read_csv(stimuli_info)\n",
    "print(f\"total number of stimuli: {len(stimuli_info)}\")\n",
    "# drop the columns that contain \"subject\"+id where id not in SUBJECT\n",
    "stimuli_info = stimuli_info.drop(columns=[col for col in stimuli_info.columns if any(f\"subject{subj}\" in col for subj in FILTERED_SUBJECT)])\n",
    "print(stimuli_info.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28580ba0",
   "metadata": {},
   "source": [
    "Load stimuli image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f926eb29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<KeysViewHDF5 ['imgBrick']>\n",
      "(73000, 425, 425, 3)\n",
      "uint8\n",
      "min: 0, max: 255\n"
     ]
    }
   ],
   "source": [
    "# load h5py\n",
    "with h5py.File(stimuli_img, 'r') as f:\n",
    "    print(f.keys())\n",
    "    images = f['imgBrick'][:]\n",
    "print(images.shape)\n",
    "print(images.dtype)\n",
    "print(f\"min: {images.min()}, max: {images.max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0259214",
   "metadata": {},
   "source": [
    "Load COCO dataset annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "920bd932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=18.13s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.83s)\n",
      "creating index...\n",
      "index created!\n",
      "{1: 'person', 2: 'bicycle', 3: 'car', 4: 'motorcycle', 5: 'airplane', 6: 'bus', 7: 'train', 8: 'truck', 9: 'boat', 10: 'traffic light', 11: 'fire hydrant', 13: 'stop sign', 14: 'parking meter', 15: 'bench', 16: 'bird', 17: 'cat', 18: 'dog', 19: 'horse', 20: 'sheep', 21: 'cow', 22: 'elephant', 23: 'bear', 24: 'zebra', 25: 'giraffe', 27: 'backpack', 28: 'umbrella', 31: 'handbag', 32: 'tie', 33: 'suitcase', 34: 'frisbee', 35: 'skis', 36: 'snowboard', 37: 'sports ball', 38: 'kite', 39: 'baseball bat', 40: 'baseball glove', 41: 'skateboard', 42: 'surfboard', 43: 'tennis racket', 44: 'bottle', 46: 'wine glass', 47: 'cup', 48: 'fork', 49: 'knife', 50: 'spoon', 51: 'bowl', 52: 'banana', 53: 'apple', 54: 'sandwich', 55: 'orange', 56: 'broccoli', 57: 'carrot', 58: 'hot dog', 59: 'pizza', 60: 'donut', 61: 'cake', 62: 'chair', 63: 'couch', 64: 'potted plant', 65: 'bed', 67: 'dining table', 70: 'toilet', 72: 'tv', 73: 'laptop', 74: 'mouse', 75: 'remote', 76: 'keyboard', 77: 'cell phone', 78: 'microwave', 79: 'oven', 80: 'toaster', 81: 'sink', 82: 'refrigerator', 84: 'book', 85: 'clock', 86: 'vase', 87: 'scissors', 88: 'teddy bear', 89: 'hair drier', 90: 'toothbrush'}\n"
     ]
    }
   ],
   "source": [
    "# load train_coco_annotations\n",
    "train_coco_annotations = COCO(train_coco_annotations)\n",
    "val_coco_annotations = COCO(val_coco_annotations)\n",
    "\n",
    "categories = train_coco_annotations.loadCats(train_coco_annotations.getCatIds())\n",
    "category_id_to_name = {cat['id']: cat['name'] for cat in categories}\n",
    "print(category_id_to_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af620be1",
   "metadata": {},
   "source": [
    "Load fMRI responses and save each fMRI trial to npy file.\n",
    "Only Need to run once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ab29ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count = 0\n",
    "for subj in SUBJECT:\n",
    "    print(f\"processing subject {subj}\")\n",
    "    session_responses = []\n",
    "    for session in range(1, NUM_SESSION+1):\n",
    "        response_file = os.path.join(\n",
    "            DATA_DIR, \n",
    "            f\"sub-{subj:02d}\",\n",
    "            PREP_TYPE,\n",
    "            f\"{PREP_NAME}_session{session:02d}.hdf5\")\n",
    "        identifier = f\"sub-{subj:02d}_ses-{session:02d}\"\n",
    "        print(identifier)\n",
    "        # load h5py\n",
    "        with h5py.File(response_file, 'r') as f:\n",
    "            responses = f[PREP_NAME][:]\n",
    "            # print(responses.shape)\n",
    "            # session_responses.append(responses)\n",
    "        print(f\"loaded {response_file}\")\n",
    "        # save subdir\n",
    "        save_dir = os.path.dirname(response_file)\n",
    "        save_dir = os.path.join(save_dir, identifier)\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        for i in range(responses.shape[0]):\n",
    "            # save the response as npy\n",
    "            np.save(os.path.join(save_dir, f\"{identifier}_res-{i:03d}.npy\"), responses[i])\n",
    "    break # break after first subject"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57258924",
   "metadata": {},
   "source": [
    "Alignment: [fmri, image, coco_id, nsd_id, label, coco_mode, sub_id]\n",
    "fMRI: 3 repetition [3, x, y, z], float16\n",
    "image: RGB format [c, h, w], uint8\n",
    "coco_id: unique id of natural stimuli in coco dataset, uint8\n",
    "nsd_id, unique id of natural stimuli in nsd, uint8\n",
    "coco_mode, train/test mode of coco, str\n",
    "sub_id, unique id of subject in nsd, str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7dd848f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adcc5302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_cocoId: 9, max_cocoId: 581929\n",
      "processing subject 1\n",
      "processing subject 2\n",
      "processing subject 5\n",
      "processing subject 7\n"
     ]
    }
   ],
   "source": [
    "# iterate over the rows in the csv file\n",
    "response_dict = {}\n",
    "# get the min and max cocoId\n",
    "min_cocoId = min(train_coco_annotations.imgToAnns.keys())\n",
    "max_cocoId = max(train_coco_annotations.imgToAnns.keys())\n",
    "print(f\"min_cocoId: {min_cocoId}, max_cocoId: {max_cocoId}\")\n",
    "\n",
    "for subject_id in SUBJECT:\n",
    "    print(f\"processing subject {subject_id}\")\n",
    "    response_dict[subject_id] = {}\n",
    "    for index, row in stimuli_info.iterrows():\n",
    "        # get the subject id\n",
    "        # if any of the subject id in the column f\"subject{subject_id}\" is not 0, then print the row\n",
    "        # find the nsd_id for the corresponding cocoId\n",
    "        if row[f\"subject{subject_id}\"] != 0:\n",
    "            response_indicies = []\n",
    "            for rep in range(0, 3):\n",
    "                response_indicies.append(row[f\"subject{subject_id}_rep{rep}\"])\n",
    "            nsd_id = row['nsdId']\n",
    "            response_dict[subject_id][nsd_id] = {}\n",
    "            response_dict[subject_id][nsd_id]['response_indicies'] = response_indicies\n",
    "            response_dict[subject_id][nsd_id]['cocoId'] = row['cocoId']\n",
    "            response_dict[subject_id][nsd_id]['coco_index'] = index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02f54b7",
   "metadata": {},
   "source": [
    "Mindeye setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85e51c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_huggingface_urls(commit='main',subj=1):\n",
    "    base_url = \"https://huggingface.co/datasets/pscotti/naturalscenesdataset/resolve/\"\n",
    "    train_url = base_url + commit + f\"/webdataset_avg_split/train/train_subj0{subj}_\" + \"{0..17}.tar\"\n",
    "    val_url = base_url + commit + f\"/webdataset_avg_split/val/val_subj0{subj}_0.tar\"\n",
    "    test_url = base_url + commit + f\"/webdataset_avg_split/test/test_subj0{subj}_\" + \"{0..1}.tar\"\n",
    "    return train_url, val_url, test_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba267dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import braceexpand\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "import webdataset as wds\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9f6c6b",
   "metadata": {},
   "source": [
    "Download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35bf7642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 1 2\n"
     ]
    }
   ],
   "source": [
    "train_url, val_url, test_url = get_huggingface_urls(\"main\",1)\n",
    "train_url = list(braceexpand.braceexpand(train_url))\n",
    "val_url = list(braceexpand.braceexpand(val_url))\n",
    "test_url = list(braceexpand.braceexpand(test_url))\n",
    "print(len(train_url), len(val_url), len(test_url))\n",
    "\n",
    "MINDEYE_NSD_DIR = os.path.join(DATA_DIR, \"mindeye\")\n",
    "os.makedirs(MINDEYE_NSD_DIR, exist_ok=True)\n",
    "if not os.path.exists(os.path.join(MINDEYE_NSD_DIR, train_url[0].rsplit('/', 1)[-1])):\n",
    "    print(\"Downloading train data...\")\n",
    "    for url in tqdm(train_url):\n",
    "        destination = MINDEYE_NSD_DIR + \"/\" + url.rsplit('/', 1)[-1]\n",
    "        print(f\"\\nDownloading {url} to {destination}...\")\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        with open(destination, 'wb') as file:\n",
    "            file.write(response.content)\n",
    "            \n",
    "    for url in tqdm(val_url):\n",
    "        destination = MINDEYE_NSD_DIR + \"/\" + url.rsplit('/', 1)[-1]\n",
    "        print(f\"\\nDownloading {url} to {destination}...\")\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        with open(destination, 'wb') as file:\n",
    "            file.write(response.content)\n",
    "            \n",
    "    for url in tqdm(test_url):\n",
    "        destination = MINDEYE_NSD_DIR + \"/\" + url.rsplit('/', 1)[-1]\n",
    "        print(f\"\\nDownloading {url} to {destination}...\")\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        with open(destination, 'wb') as file:\n",
    "            file.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84b06cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start processing train data\n",
      "start processing val data\n",
      "start processing test data\n",
      "num of train coco ids: 8559, num of val coco ids: 300, num of test coco ids: 982\n"
     ]
    }
   ],
   "source": [
    "# train_url = train_url[local_rank:world_size]\n",
    "batch_size = 1\n",
    "num_worker_batches = 4\n",
    "num_train = 8559 + 300\n",
    "voxels_key=\"nsdgeneral.npy\"\n",
    "to_tuple=[\"voxels\", \"images\", \"coco\"]\n",
    "train_data = wds.WebDataset(train_url, resampled=False, cache_dir=MINDEYE_NSD_DIR)\\\n",
    "    .decode(\"torch\")\\\n",
    "    .rename(images=\"jpg;png\", voxels=voxels_key, trial=\"trial.npy\", coco=\"coco73k.npy\", reps=\"num_uniques.npy\")\\\n",
    "    .to_tuple(*to_tuple)\\\n",
    "    .batched(batch_size, partial=True)\n",
    "\n",
    "val_data = wds.WebDataset(val_url, resampled=False, cache_dir=MINDEYE_NSD_DIR)\\\n",
    "    .decode(\"torch\")\\\n",
    "    .rename(images=\"jpg;png\", voxels=voxels_key, trial=\"trial.npy\", coco=\"coco73k.npy\", reps=\"num_uniques.npy\")\\\n",
    "    .to_tuple(*to_tuple)\\\n",
    "    .batched(batch_size, partial=True)\n",
    "\n",
    "test_data = wds.WebDataset(test_url, resampled=False, cache_dir=MINDEYE_NSD_DIR)\\\n",
    "    .decode(\"torch\")\\\n",
    "    .rename(images=\"jpg;png\", voxels=voxels_key, trial=\"trial.npy\", coco=\"coco73k.npy\", reps=\"num_uniques.npy\")\\\n",
    "    .to_tuple(*to_tuple)\\\n",
    "    .batched(batch_size, partial=True)\n",
    "    \n",
    "train_coco_ids, val_coco_ids, test_coco_ids = [], [], []\n",
    "print(f\"start processing train data\")\n",
    "for i, batch in enumerate(train_data):\n",
    "    voxels_, images_, nsd_id = batch\n",
    "    # print(voxels.shape)\n",
    "    # print(images.shape)\n",
    "    # print(coco_id.shape)\n",
    "    nsd_id = nsd_id[0][0]\n",
    "    # find coco_id in stimuli_info\n",
    "    coco_id = stimuli_info[stimuli_info['nsdId'] == nsd_id]['cocoId'].values[0]\n",
    "    # print(f\"nsd_id: {nsd_id}\")\n",
    "    \n",
    "    # # get the coco annotation\n",
    "    # coco_annotations = train_coco_annotations.imgToAnns[coco_id]\n",
    "    # if len(coco_annotations) == 0:\n",
    "    #     coco_annotations = val_coco_annotations.imgToAnns[coco_id]\n",
    "    # print(coco_annotations)\n",
    "    # # get text of coco_annotations\n",
    "    # text_annotations = []\n",
    "    # for ann in coco_annotations:\n",
    "    #     text_annotations.append(category_id_to_name[ann['category_id']])\n",
    "    # image = images[0].permute(1, 2, 0)\n",
    "    # plt.imshow(image)\n",
    "    # plt.title(text_annotations)\n",
    "    # plt.show()\n",
    "    # break\n",
    "    train_coco_ids.append(coco_id)\n",
    "    \n",
    "print(f\"start processing val data\")\n",
    "for i, batch in enumerate(val_data):\n",
    "    voxels_, images_, nsd_id = batch\n",
    "    nsd_id = nsd_id[0][0]\n",
    "    coco_id = stimuli_info[stimuli_info['nsdId'] == nsd_id]['cocoId'].values[0]\n",
    "    val_coco_ids.append(coco_id)\n",
    "\n",
    "print(f\"start processing test data\")\n",
    "for i, batch in enumerate(test_data):\n",
    "    voxels_, images_, nsd_id = batch\n",
    "    nsd_id = nsd_id[0][0]\n",
    "    coco_id = stimuli_info[stimuli_info['nsdId'] == nsd_id]['cocoId'].values[0]\n",
    "    test_coco_ids.append(coco_id)\n",
    "\n",
    "print(f\"num of train coco ids: {len(train_coco_ids)}, num of val coco ids: {len(val_coco_ids)}, num of test coco ids: {len(test_coco_ids)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4462b305",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>cocoId</th>\n",
       "      <th>cocoSplit</th>\n",
       "      <th>cropBox</th>\n",
       "      <th>loss</th>\n",
       "      <th>nsdId</th>\n",
       "      <th>flagged</th>\n",
       "      <th>BOLD5000</th>\n",
       "      <th>shared1000</th>\n",
       "      <th>subject1</th>\n",
       "      <th>...</th>\n",
       "      <th>subject1_rep2</th>\n",
       "      <th>subject2_rep0</th>\n",
       "      <th>subject2_rep1</th>\n",
       "      <th>subject2_rep2</th>\n",
       "      <th>subject5_rep0</th>\n",
       "      <th>subject5_rep1</th>\n",
       "      <th>subject5_rep2</th>\n",
       "      <th>subject7_rep0</th>\n",
       "      <th>subject7_rep1</th>\n",
       "      <th>subject7_rep2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>532481</td>\n",
       "      <td>val2017</td>\n",
       "      <td>(0, 0, 0.1671875, 0.1671875)</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>245764</td>\n",
       "      <td>val2017</td>\n",
       "      <td>(0, 0, 0.125, 0.125)</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13985</td>\n",
       "      <td>14176</td>\n",
       "      <td>28603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>385029</td>\n",
       "      <td>val2017</td>\n",
       "      <td>(0, 0, 0.125, 0.125)</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>311303</td>\n",
       "      <td>val2017</td>\n",
       "      <td>(0, 0, 0.16640625, 0.16640625)</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>393226</td>\n",
       "      <td>val2017</td>\n",
       "      <td>(0, 0, 0.125, 0.125)</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72995</th>\n",
       "      <td>72995</td>\n",
       "      <td>518071</td>\n",
       "      <td>train2017</td>\n",
       "      <td>(0, 0, 0.125, 0.125)</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>72995</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72996</th>\n",
       "      <td>72996</td>\n",
       "      <td>255930</td>\n",
       "      <td>train2017</td>\n",
       "      <td>(0, 0, 0.125, 0.125)</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>72996</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72997</th>\n",
       "      <td>72997</td>\n",
       "      <td>255934</td>\n",
       "      <td>train2017</td>\n",
       "      <td>(0, 0, 0.1, 0.1)</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>72997</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72998</th>\n",
       "      <td>72998</td>\n",
       "      <td>518080</td>\n",
       "      <td>train2017</td>\n",
       "      <td>(0.125, 0.125, 0, 0)</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>72998</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5585</td>\n",
       "      <td>11846</td>\n",
       "      <td>14495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72999</th>\n",
       "      <td>72999</td>\n",
       "      <td>518083</td>\n",
       "      <td>train2017</td>\n",
       "      <td>(0, 0, 0.16640625, 0.16640625)</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>72999</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>22664</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>73000 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0  cocoId  cocoSplit                         cropBox  \\\n",
       "0               0  532481    val2017    (0, 0, 0.1671875, 0.1671875)   \n",
       "1               1  245764    val2017            (0, 0, 0.125, 0.125)   \n",
       "2               2  385029    val2017            (0, 0, 0.125, 0.125)   \n",
       "3               3  311303    val2017  (0, 0, 0.16640625, 0.16640625)   \n",
       "4               4  393226    val2017            (0, 0, 0.125, 0.125)   \n",
       "...           ...     ...        ...                             ...   \n",
       "72995       72995  518071  train2017            (0, 0, 0.125, 0.125)   \n",
       "72996       72996  255930  train2017            (0, 0, 0.125, 0.125)   \n",
       "72997       72997  255934  train2017                (0, 0, 0.1, 0.1)   \n",
       "72998       72998  518080  train2017            (0.125, 0.125, 0, 0)   \n",
       "72999       72999  518083  train2017  (0, 0, 0.16640625, 0.16640625)   \n",
       "\n",
       "           loss  nsdId  flagged  BOLD5000  shared1000  subject1  ...  \\\n",
       "0      0.100000      0    False     False       False         0  ...   \n",
       "1      0.000000      1    False     False       False         0  ...   \n",
       "2      0.000000      2    False     False       False         0  ...   \n",
       "3      0.125000      3    False     False       False         0  ...   \n",
       "4      0.133333      4    False     False       False         0  ...   \n",
       "...         ...    ...      ...       ...         ...       ...  ...   \n",
       "72995  0.000000  72995    False     False       False         0  ...   \n",
       "72996  0.125000  72996    False     False       False         0  ...   \n",
       "72997  0.000000  72997    False     False       False         0  ...   \n",
       "72998  0.000000  72998    False     False       False         0  ...   \n",
       "72999  0.000000  72999    False     False       False         1  ...   \n",
       "\n",
       "       subject1_rep2  subject2_rep0  subject2_rep1  subject2_rep2  \\\n",
       "0                  0              0              0              0   \n",
       "1                  0              0              0              0   \n",
       "2                  0              0              0              0   \n",
       "3                  0              0              0              0   \n",
       "4                  0              0              0              0   \n",
       "...              ...            ...            ...            ...   \n",
       "72995              0              0              0              0   \n",
       "72996              0              0              0              0   \n",
       "72997              0              0              0              0   \n",
       "72998              0              0              0              0   \n",
       "72999          22664              0              0              0   \n",
       "\n",
       "       subject5_rep0  subject5_rep1  subject5_rep2  subject7_rep0  \\\n",
       "0                  0              0              0              0   \n",
       "1                  0              0              0          13985   \n",
       "2                  0              0              0              0   \n",
       "3                  0              0              0              0   \n",
       "4                  0              0              0              0   \n",
       "...              ...            ...            ...            ...   \n",
       "72995              0              0              0              0   \n",
       "72996              0              0              0              0   \n",
       "72997              0              0              0              0   \n",
       "72998              0              0              0           5585   \n",
       "72999              0              0              0              0   \n",
       "\n",
       "       subject7_rep1  subject7_rep2  \n",
       "0                  0              0  \n",
       "1              14176          28603  \n",
       "2                  0              0  \n",
       "3                  0              0  \n",
       "4                  0              0  \n",
       "...              ...            ...  \n",
       "72995              0              0  \n",
       "72996              0              0  \n",
       "72997              0              0  \n",
       "72998          11846          14495  \n",
       "72999              0              0  \n",
       "\n",
       "[73000 rows x 25 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stimuli_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b1091b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing subject 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coco_id 190756 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 111036 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 512194 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 50165 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 106389 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 524470 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 524486 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 3348 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 527643 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 528116 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 44592 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 5962 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 531828 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 8998 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 534336 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 11358 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 273979 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 11925 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 14128 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 278714 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 279279 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 20517 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 47498 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 548670 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 26622 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 289170 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 553166 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 295889 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 299254 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 563914 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 303178 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 41458 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 565906 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 42622 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 43486 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 132395 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 46014 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 47746 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 48388 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 575249 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 313659 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 575929 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 358134 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 533224 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 412062 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 54214 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 580765 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 61259 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 66502 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 335197 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 337030 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 231103 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 338521 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 79445 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 82775 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 345797 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 349038 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 87126 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 88092 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 91670 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 353915 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 93603 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 358345 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 540347 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 361448 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 361994 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 102625 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 106047 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 106470 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 106487 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 108582 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 372165 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 373509 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 106226 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 115075 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 115887 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 379259 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 379853 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 119057 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 382554 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 383324 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 107959 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 123692 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 123891 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 387338 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 387850 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 390685 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 132987 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 415434 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 134075 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 134460 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 139198 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 198195 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 147818 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 410388 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 151161 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 414853 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 416651 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 275775 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 163598 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 426269 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 426513 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 428163 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 14269 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 432170 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 434066 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 436162 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 437660 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 439493 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 440136 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 178423 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 442489 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 181711 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 447091 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 6178 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 452441 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 452454 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 512339 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 190906 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 556332 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 194698 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 459347 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 197573 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 200374 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 202050 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 464602 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 203177 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 33912 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 466591 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 467318 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 296474 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 471056 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 210804 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 211303 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 211644 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 556982 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 298370 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 480196 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 219653 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 386856 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 489950 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 227851 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 82005 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 494743 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 494900 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 497804 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 236075 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 237844 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 242544 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 242607 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 505849 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 506183 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 246044 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 246563 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 565693 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 511198 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 249131 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 512282 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "coco_id 516404 is not in train_coco_ids, val_coco_ids, or test_coco_ids\n",
      "num_train: 8559, num_val: 300, num_test: 982\n"
     ]
    }
   ],
   "source": [
    "num_train, num_val, num_test = 0, 0, 0\n",
    "# concat all response_indicies into a single array\n",
    "for subject_id in response_dict.keys():\n",
    "    print(f\"processing subject {subject_id}\")\n",
    "    response_array = []\n",
    "    for nsd_id in response_dict[subject_id].keys():\n",
    "        response_array.append(response_dict[subject_id][nsd_id]['response_indicies'])\n",
    "    response_array = np.concatenate(response_array, axis=0)\n",
    "    assert len(response_array) == 30000, f\"response_array length is {len(response_array)}\"\n",
    "    \n",
    "\n",
    "    for nsd_id in response_dict[subject_id].keys():\n",
    "        sample_dict = {}\n",
    "        response_indicies = response_dict[subject_id][nsd_id]['response_indicies']\n",
    "        # get the fmri response\n",
    "        fmri = []\n",
    "        for i in range(len(response_indicies)):\n",
    "            response_index = response_indicies[i]\n",
    "            # use divmod\n",
    "            session_id, response_id = divmod(response_index, 750)\n",
    "            if response_id == 0:\n",
    "                response_id = 750\n",
    "                session_id -= 1\n",
    "            response_id -= 1\n",
    "            session_id += 1\n",
    "            identifier = f\"sub-{subject_id:02d}_ses-{session_id:02d}\"\n",
    "            response_file = os.path.join(\n",
    "                DATA_DIR, \n",
    "                f\"sub-{subject_id:02d}\",\n",
    "                PREP_TYPE,\n",
    "                identifier,\n",
    "                f\"{identifier}_res-{response_id:03d}.npy\")\n",
    "            fmri_response = np.load(response_file) # [x, y, z]\n",
    "            fmri.append(fmri_response)\n",
    "        # concat fmri to [N, x, y, z]\n",
    "        fmri = np.stack(fmri, axis=0)\n",
    "        sample_dict['fmri'] = fmri\n",
    "        # get coco_id\n",
    "        coco_id = response_dict[subject_id][nsd_id]['cocoId']\n",
    "        sample_dict['cocoId'] = coco_id\n",
    "\n",
    "        if coco_id in train_coco_ids:\n",
    "            sub_dir = \"train\"\n",
    "            num_train += 1\n",
    "        elif coco_id in val_coco_ids:\n",
    "            sub_dir = \"val\"\n",
    "            num_val += 1\n",
    "        elif coco_id in test_coco_ids:\n",
    "            sub_dir = \"test\"\n",
    "            num_test += 1\n",
    "        else:\n",
    "            print(f\"coco_id {coco_id} is not in train_coco_ids, val_coco_ids, or test_coco_ids\")\n",
    "            continue\n",
    "        # get the coco annotation\n",
    "        coco_annotations = train_coco_annotations.imgToAnns[coco_id]\n",
    "        if len(coco_annotations) == 0:\n",
    "            coco_annotations = val_coco_annotations.imgToAnns[coco_id]\n",
    "        text_annotations = []\n",
    "        for ann in coco_annotations:\n",
    "            text_annotations.append(category_id_to_name[ann['category_id']])\n",
    "        sample_dict['label'] = text_annotations\n",
    "        # get the coco image\n",
    "        coco_image = images[response_dict[subject_id][nsd_id]['coco_index']]\n",
    "        sample_dict['image'] = coco_image\n",
    "        # get the fmri response indicies\n",
    "        sample_dict['response_indicies'] = response_indicies\n",
    "        # get the nsdId and subjectId\n",
    "        sample_dict['nsdId'] = nsd_id\n",
    "        sample_dict['subjectId'] = subject_id\n",
    "\n",
    "        # save the sample_dict as npy\n",
    "        save_dir = os.path.join(OUTPUT_DIR, f\"sub-{subject_id:02d}\", sub_dir)\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        np.save(os.path.join(save_dir, f\"sample_dict_{nsd_id}.npy\"), sample_dict)\n",
    "        # break\n",
    "        \n",
    "    break # break after first subject\n",
    "print(f\"num_train: {num_train}, num_val: {num_val}, num_test: {num_test}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
